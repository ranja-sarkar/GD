# gradient descent

Vanilla gradient descent, also called **batch gradient descent** has two variants namely, mini-batch gradient descent and **stochastic gradient descent (SGD)**.

**Chapter 4** of my book discusses these variants of gradient descent alongwith the model hyper-parameter 'learning rate' in detail.

<img width="115" alt="0" src="https://github.com/user-attachments/assets/c86f23ab-7707-408b-bfc7-ef5cddabdc58">

Buy at Amazon: https://a.co/d/gw13Tv6


Three linear regression models optimized with the gradient descent algorithm with different learning rates have been shown. It tells us that with a too low learning rate, the algorithm may reach the 
maximum permissible number of iterations before reaching the minimum point, whereas it may not converge (or may diverge completely) with a too high learning rate. Selecting the appropriate learning rate is crucial in achieving an optimally performing model.

<img width="316" alt="1" src="https://github.com/user-attachments/assets/453a37ea-469a-4373-8a79-ad2589a2e957">

<img width="320" alt="2" src="https://github.com/user-attachments/assets/5c2e2d4a-82b1-4fbf-bf7a-fd5cb6d40c1e">

<img width="308" alt="3" src="https://github.com/user-attachments/assets/69659f9d-42f7-47e1-88a7-b818b5ad93c6">

<img width="149" alt="0" src="https://github.com/user-attachments/assets/71f249d0-1df0-429f-843a-9ac8936d3d92">


Additionally, refer to my article on the hyperparameters 'learning rate', 'epoch', and 'batch' in context of neural networks (deep learning):
https://open.substack.com/pub/ranjas/p/hyper-parameters-of-a-neural-network?utm_campaign=post&utm_medium=web

