# gradient descent

Vanilla gradient descent, also called **batch gradient descent** has two variants namely, mini-batch gradient descent and **stochastic gradient descent (SGD)**.

**Chapter 4** of my book discusses these variants of gradient descent alongwith the model hyper-parameter 'learning rate' in detail.

<img width="115" alt="0" src="https://github.com/user-attachments/assets/c86f23ab-7707-408b-bfc7-ef5cddabdc58">

Additionally, refer to my article on the hyperparameters 'learning rate', 'epoch', and 'batch' in context of neural networks (deep learning):
https://open.substack.com/pub/ranjas/p/hyper-parameters-of-a-neural-network?utm_campaign=post&utm_medium=web

