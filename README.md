# gradient descent

Vanilla gradient descent, also called **batch gradient descent** has two variants namely, mini-batch gradient descent and **stochastic gradient descent (SGD)**.

**Chapter 4** of my book discusses these variants of gradient descent alongwith the model hyper-parameter 'learning rate' in detail.

<img width="115" alt="0" src="https://github.com/user-attachments/assets/c86f23ab-7707-408b-bfc7-ef5cddabdc58">

Three linear regression models optimized with the gradient descent algorithm with different learning rates have been shown:

<img width="316" alt="1" src="https://github.com/user-attachments/assets/453a37ea-469a-4373-8a79-ad2589a2e957">

<img width="320" alt="2" src="https://github.com/user-attachments/assets/5c2e2d4a-82b1-4fbf-bf7a-fd5cb6d40c1e">

Additionally, refer to my article on the hyperparameters 'learning rate', 'epoch', and 'batch' in context of neural networks (deep learning):
https://open.substack.com/pub/ranjas/p/hyper-parameters-of-a-neural-network?utm_campaign=post&utm_medium=web

